{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9356ca0f-ee2c-4796-9c1e-f9968f146783",
   "metadata": {},
   "source": [
    "# Overview \n",
    "\n",
    "In this notebook Llama 3.1 8b will be fine-tuned to play chess. The model will be pulled from the Sagemaker Jumpstart model hub and fine-tuned using a Sagemaker training job. Sagemaker Jumpstart is a model hub with 400+ LLM's that can be deployed as is or fine-tuned. An overview of Llama 3.1 on Sagemaker Jumpstart can be found [here](https://aws.amazon.com/blogs/machine-learning/meta-llama-3-1-models-are-now-available-in-amazon-sagemaker-jumpstart/). \n",
    "\n",
    "This notebook can be run on any environment of the user's choice as the training compute will be offloaded into a \"ml.g5.24xlarge\". If this notebook is run outside of an Amazon Sagemaker environment, please ensure the AWS user credentials are correctly initialized. Some environments this notebook can be run on some examples:\n",
    "\n",
    "1. Configuring an AWS EC2 instance with a Deep Learning AMI, and setting up a Jupyter Server: Link\n",
    "2. Configuring an Amazon Sagemaker environment: Link\n",
    "3. Configure your own environment, with adequate compute to run this notebook.\n",
    "\n",
    "The Chess moves dataset is pulled from [here](https://www.pgnmentor.com/) under players --> Carlsen.  This dataset outlines the board state in FEN notation, and states the next legal move based on the board state & player turn. More information about what FEN notation is and how to interpret it can be found [here](https://www.chess.com/terms/fen-chess).\n",
    "\n",
    "Once the model is imported at the end of this notebook, please open \"test_chess_model.ipynb\" to use the model to play chess. If this is the intention skip the \"Clean up\" section at the bottom of this notebook.\n",
    "\n",
    "**_NOTE:_** This notebook was tested in the us-east-1 region of AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023fdae-56b9-48f0-b641-a11b0f6ff281",
   "metadata": {},
   "source": [
    "## Amazon Bedrock Custom Model Import (CMI)\n",
    "\n",
    "The resulting model files are imported into Amazon Bedrock via Custom Model Import (CMI).\n",
    "\n",
    "Bedrock Custom Model Import allows for importing foundation models that have been customized in other environments outside of Amazon Bedrock, such as Amazon Sagemaker, EC2, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e25183-991a-4447-8c33-c4faaa60622b",
   "metadata": {},
   "source": [
    "## What will you learn from this notebook\n",
    "\n",
    "In this notebook, you will learn how to:\n",
    "\n",
    "* Pull a model from Sagemaker Jumpstart & finetune it \n",
    "* Use a custom dataset & process it to conform to a prompt template of choice\n",
    "* Finetune the model (Llama 3.1 8b) using Sagemaker training jobs\n",
    "* Deploy the finetuned model to Amazon Bedrock Custom Import & Conduct Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca9e3ff-de16-4d9e-954c-d8b87dbfdb35",
   "metadata": {},
   "source": [
    "## Architectural Pattern\n",
    "\n",
    "![Diagram](./images/architecture.png \"Architecture\")\n",
    "\n",
    "As can be seen from the diagram above, the model (Llama 3.1 8b) gets pulled from Sagemaker Jumpstart, and The dataset is a chess dataset from pgnmentor.com mirroring Magnus Carlson's games. The model files are then stored in an S3 bucket, to then be imported into Amazon Bedrock. This architecture is modular because the Notebook can be run anywhere, that the appropriate compute is available (as explained earlier)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710d1bf7-49d5-4fd3-b0bb-93c3dc14a556",
   "metadata": {},
   "source": [
    "## Code with comments "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffbd9ec-4dec-476a-b971-3ef2aac6b91b",
   "metadata": {},
   "source": [
    "### Install dependencies & restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e4c118a-68f5-4d97-ba10-4933336d17b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
      "The folder you are executing pip from can no longer be found.\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3\n",
    "!pip install botocore\n",
    "!pip install -U 'aiobotocore[boto3]' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e9c4ddc-72f1-4338-b814-e5362fb6ed39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools==69.0.2\n",
      "  Using cached setuptools-69.0.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Using cached setuptools-69.0.2-py3-none-any.whl (819 kB)\n",
      "Installing collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.6.0\n",
      "    Uninstalling setuptools-75.6.0:\n",
      "      Successfully uninstalled setuptools-75.6.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dash 2.17.1 requires dash-core-components==2.0.0, which is not installed.\n",
      "dash 2.17.1 requires dash-html-components==2.0.0, which is not installed.\n",
      "dash 2.17.1 requires dash-table==5.0.0, which is not installed.\n",
      "autogluon-common 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-core 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-core 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-features 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-features 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed setuptools-69.0.2\n",
      "Requirement already satisfied: chess in /opt/conda/lib/python3.10/site-packages (1.11.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install setuptools==69.0.2\n",
    "!pip install chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30660db-e035-46cd-bc07-6fb7282c5336",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dash 2.17.1 requires dash-core-components==2.0.0, which is not installed.\n",
      "dash 2.17.1 requires dash-html-components==2.0.0, which is not installed.\n",
      "dash 2.17.1 requires dash-table==5.0.0, which is not installed.\n",
      "autogluon-common 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-core 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-core 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-features 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-features 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools wheel --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5744b166-8103-45e2-a5db-12523f4866e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.40.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "656af867-994c-4216-a063-b714f7f2777c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a45ae70-e8b3-4c6f-bd0f-6691596ac3f0",
   "metadata": {},
   "source": [
    "### Setup Sagemaker client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1925d8cc-e1b9-4c91-97ca-0f731d027051",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Sagemaker Version - 2.225.0\n",
      "Transformers version - 4.40.2\n",
      "sagemaker role arn: arn:aws:iam::874604298668:role/service-role/AmazonSageMaker-ExecutionRole-20240122T092140\n",
      "sagemaker bucket: sagemaker-us-east-1-874604298668\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import transformers\n",
    "print(f\"Sagemaker Version - {sagemaker.__version__}\")\n",
    "print(f\"Transformers version - {transformers.__version__}\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    " \n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    " \n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    " \n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5b4984-d169-40aa-a2a3-283c3c8edd49",
   "metadata": {},
   "source": [
    "### Identify model id from Sagemaker Jumpstart & Setup data location\n",
    "Sagemaker jumpstart has unique identifiers for each model present. Below is the model id & the model version for Llama 3.1 8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a85416e-71a5-4c4f-81d9-0f90a97a7d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "model_id, model_version = \"meta-textgeneration-llama-3-1-8b\", \"2.2.2\"\n",
    "pretrained_model = JumpStartModel(model_id=model_id, model_version=model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c975db8d-dc68-4ca0-bbd9-9188cada66d6",
   "metadata": {},
   "source": [
    "In the cell below the training data location is being specified to the Sagemaker default S3 bucket. In addition the local training/validation paths are being identified to store in the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed25e4e1-1e1f-40b0-ac9f-019855a01b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_test_data_location = f's3://{sess.default_bucket()}/datasets/3-1-8b'\n",
    "local_train_data_file = \"data/train.jsonl\"\n",
    "local_test_data_file = \"data/validation.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784f63be-a997-46eb-ae73-4221bba7521a",
   "metadata": {},
   "source": [
    "## Dataset Download section \n",
    "\n",
    "the dataset is being downloaded from [pgnmentor.com](https://www.pgnmentor.com/) this website allows for the extensive viewing & analysis of previously played chess games. The data that will be used to fine tune this model will be data from some of Magnus Carlsen's games. The cells below outline the process of downloading the data to the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3988cd8b-a22d-450a-9362-936aff226376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1378k  100 1378k    0     0  5475k      0 --:--:-- --:--:-- --:--:-- 5493k\n"
     ]
    }
   ],
   "source": [
    "# Download Carlsen's games zip file to the target directory\n",
    "!curl -o ./data/Carlsen.zip https://www.pgnmentor.com/players/Carlsen.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01e85449-9a69-4d1c-b45c-ab8ee3531917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./data/Carlsen.zip\n",
      "  inflating: ./data/Carlsen.pgn      \n"
     ]
    }
   ],
   "source": [
    "# Unzip the file in the target directory\n",
    "!unzip ./data/Carlsen.zip -d ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1d90ee2-a1f4-4e21-9219-17b925040a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the unzipped folder to 'top'\n",
    "!mv ./data/Carlsen.pgn ./data/top.pgn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0764c15-0d97-46f4-9250-4be38201efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the zip file after unzipping\n",
    "!rm ./data/Carlsen.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa65b0-93b3-45cf-a3cb-79ec4c695d7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset Processing & Validation \n",
    "In the cell below the data is being processed to fit the correct format, and being saved to the local directory.\n",
    "Some points of importance in the cell below:\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0e21013-505a-4b95-870b-495ae9e3512b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File being processed - data/top.pgn\n",
      "Reached 10000 records. Stopping.\n",
      "Total records written: 10014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a890f89ec4c3433d917717230f479831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057024ab450c43148a6f4bbd8789d044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/7009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab1c281f8cd4d06afeb98bfc57b08e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/3005 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New schema for dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['move', 'fen', 'nxt-color'],\n",
      "        num_rows: 7009\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['move', 'fen', 'nxt-color'],\n",
      "        num_rows: 3005\n",
      "    })\n",
      "})\n",
      "\n",
      "Dataset sizes:\n",
      "train: 7009 samples\n",
      "test: 3005 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb8eb41e6d547afab9a19d2f3f30789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36ff3dbb04741e5bbbbdbca3a24fe95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3005 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012bb2a9b4be4dac8e19e041ee5a5bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da462d44920d42f5a03b62efc9f079f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "326791"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chess.pgn\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "import random\n",
    "\n",
    "def create_chess_message(fen, next_move_color, best_move_san=None):\n",
    "    instruction = \"You are a chess engine. Analyze the given position and generate the next best valid move in SAN format.\"\n",
    "    return {\n",
    "        \"instruction\": instruction,\n",
    "        \"fen\": fen,\n",
    "        \"nxt-color\": next_move_color,\n",
    "        \"move\": best_move_san if best_move_san else None\n",
    "    }\n",
    "\n",
    "def process_sample(sample):\n",
    "    return create_chess_message(\n",
    "        fen=sample['fen'],\n",
    "        next_move_color=sample['nxt_color'],\n",
    "        best_move_san=sample.get('move')\n",
    "    )\n",
    "\n",
    "# Initialize a counter for the number of records written (set to 35000000 for the full dataset)\n",
    "#set to 100 by default to finish the training job fast\n",
    "records_written = 0\n",
    "max_records = 10000\n",
    "\n",
    "# Your existing code for creating data.json remains the same\n",
    "pathlist = Path(\"data/\").glob('**/*.pgn')\n",
    "with open(\"data/data.json\", 'w') as f:\n",
    "    for path in pathlist:\n",
    "        print(f'File being processed - {path}')\n",
    "        pgn = open(path)\n",
    "        while True:\n",
    "            if records_written >= max_records:\n",
    "                print(f\"Reached {max_records} records. Stopping.\")\n",
    "                break\n",
    "            try:\n",
    "                game = chess.pgn.read_game(pgn)\n",
    "                if game is None:\n",
    "                    break\n",
    "                else:\n",
    "                    result = game.headers[\"Result\"]\n",
    "                    if result == \"1-0\":\n",
    "                        winner = \"WHITE\"\n",
    "                    elif result == \"0-1\":\n",
    "                        winner = \"BLACK\"\n",
    "                    else:\n",
    "                        winner = \"DRAW\"\n",
    "\n",
    "                    board = chess.Board()  # Create a new board for each game\n",
    "                    for move in game.mainline_moves():\n",
    "                        current_color = \"WHITE\" if board.turn else \"BLACK\"\n",
    "                        if winner == \"DRAW\" or current_color == winner: #consider the moves only if it's a winner's move or neutral\n",
    "                            if move in board.legal_moves:\n",
    "                                move_json = {\n",
    "                                    \"move\": board.san(move),\n",
    "                                    \"fen\": board.fen(),\n",
    "                                    \"nxt_color\": current_color,\n",
    "                                }\n",
    "                                f.write(json.dumps(move_json) + \"\\n\")\n",
    "                                records_written += 1\n",
    "                                board.push(move)\n",
    "                            else:\n",
    "                                print(f\"Illegal move encountered: {board.san(move)} in position {board.fen()}. Skipping this move.\")\n",
    "                        else:\n",
    "                            board.push(move)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing game: {str(e)}. Skipping this game.\")\n",
    "\n",
    "        if records_written >= max_records:\n",
    "            break\n",
    "            \n",
    "print(f\"Total records written: {records_written}\")\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"data/data.json\", split=\"train[:100%]\")\n",
    "\n",
    "# Create a 70/30 train/test split\n",
    "dataset = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "\n",
    "# Shuffle each split\n",
    "for split in dataset:\n",
    "    dataset[split] = dataset[split].shuffle(seed=42)\n",
    "\n",
    "# Apply the processing function to each split\n",
    "dataset = dataset.map(\n",
    "    process_sample,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    "    num_proc=os.cpu_count()  # Use multiple processes for faster processing\n",
    ")\n",
    "\n",
    "dataset = dataset.remove_columns(\"instruction\") # Not needed as we are adding the instruction in template\n",
    "\n",
    "print(f'New schema for dataset: {dataset}')\n",
    "print(f'\\nDataset sizes:')\n",
    "for split in dataset:\n",
    "    print(f'{split}: {len(dataset[split])} samples')\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "# Save the processed datasets\n",
    "dataset['train'].save_to_disk(\"data/processed_chess_train\")\n",
    "dataset['test'].save_to_disk(\"data/processed_chess_test\")\n",
    "\n",
    "dataset[\"train\"].to_json(local_train_data_file, orient=\"records\", force_ascii=False)\n",
    "dataset[\"test\"].to_json(local_test_data_file, orient=\"records\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d29c7-7ab1-4f23-9ef2-f2869b191a99",
   "metadata": {},
   "source": [
    "Using the Python Chess library, the dataset is being validated to ensure all recommended moves are valid. This will ensure the LLM has 100% true examples to learn from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6d33250-d0ab-4daa-978a-0bb022b832be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating moves: 100%|██████████| 10014/10014 [00:01<00:00, 6721.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 10014\n",
      "Valid moves: 10014\n",
      "Invalid moves: 0\n",
      "All moves are valid!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Validating the dataset created has all valid and legal moves only - for the safer side!\n",
    "import json\n",
    "import chess\n",
    "from tqdm import tqdm\n",
    "\n",
    "def validate_move(fen, move_san):\n",
    "    try:\n",
    "        board = chess.Board(fen)\n",
    "        # Parse SAN move and validate it's legal\n",
    "        move = board.parse_san(move_san)\n",
    "        return move in board.legal_moves\n",
    "    except ValueError:\n",
    "        # If the move can't be parsed in SAN format\n",
    "        return False\n",
    "\n",
    "def validate_data_json(file_path):\n",
    "    valid_count = 0\n",
    "    invalid_count = 0\n",
    "    invalid_records = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for i, line in enumerate(tqdm(lines, desc=\"Validating moves\")):\n",
    "        record = json.loads(line)\n",
    "        fen = record['fen']\n",
    "        move = record['move']\n",
    "\n",
    "        try:\n",
    "            if validate_move(fen, move):\n",
    "                valid_count += 1\n",
    "            else:\n",
    "                invalid_count += 1\n",
    "                invalid_records.append((i, fen, move))\n",
    "        except Exception as e:\n",
    "            # Handle any other unexpected errors\n",
    "            invalid_count += 1\n",
    "            invalid_records.append((i, fen, f\"{move} (Error: {str(e)})\"))\n",
    "\n",
    "    print(f\"Total records: {len(lines)}\")\n",
    "    print(f\"Valid moves: {valid_count}\")\n",
    "    print(f\"Invalid moves: {invalid_count}\")\n",
    "\n",
    "    if invalid_records:\n",
    "        print(\"\\nInvalid records:\")\n",
    "        for record in invalid_records[:10]:  # Print first 10 invalid records\n",
    "            print(f\"Line {record[0]}: FEN: {record[1]}, Move: {record[2]}\")\n",
    "        \n",
    "        if len(invalid_records) > 10:\n",
    "            print(f\"... and {len(invalid_records) - 10} more.\")\n",
    "\n",
    "    return valid_count == len(lines)\n",
    "\n",
    "# Run the validation\n",
    "file_path = \"data/data.json\"\n",
    "all_valid = validate_data_json(file_path)\n",
    "\n",
    "if all_valid:\n",
    "    print(\"All moves are valid!\")\n",
    "else:\n",
    "    print(\"Some moves are invalid. Please check the output above for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f7726-0085-4153-b8fe-3e62607f98a3",
   "metadata": {},
   "source": [
    "Print random samples from the dataset to view the cleaned & prepped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "356f8cec-ce7d-4751-924a-eb3ae421259d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random samples from train set:\n",
      "r2qr1k1/pp3ppp/5n2/3p1b2/2nB4/1NP2PP1/P1P2QBP/R3R1K1 b - - 8 17\n",
      "rnbq1rk1/ppp1ppbp/3p1np1/8/2PP4/5NP1/PP2PPBP/RNBQ1RK1 b - - 0 6\n",
      "\n",
      "Random samples from test set:\n",
      "r1b1q3/p2p2kp/np1P1rp1/1N1R1n2/2BQ4/P3P3/1PP4P/2K2R2 w - - 0 31\n",
      "6k1/4qpb1/1p2p1p1/p1n4p/P1P2P2/1P3BP1/5BKP/3Q4 w - - 6 44\n"
     ]
    }
   ],
   "source": [
    "# Print random samples\n",
    "for split in ['train', 'test']:\n",
    "    print(f\"\\nRandom samples from {split} set:\")\n",
    "    for index in random.sample(range(len(dataset[split])), 2):\n",
    "        print(dataset[split][index][\"fen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f32cee-a1a3-4e4e-aeb2-3cb2eecbb42d",
   "metadata": {},
   "source": [
    "Create the prompt template. This template is needed by Sagemaker Jumpstart to understand the given template from the dataset. This file must be in the same directory as the training data, and named \"template.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79f5c469-977d-4d9e-aa7e-57563a8e32b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": (\n",
    "        \"<s>[INST] You are a chess engine. Given a chess position in FEN notation and the color to move, \"\n",
    "        \"provide the next best valid move in SAN (Standard Algebraic Notation) format to progress towards winning the game of chess. \"\n",
    "        \"Your response must be a single move wrapped in <move></move> tags.\\n\\n\"\n",
    "        \"Chess Position (FEN): {fen}\\n\"\n",
    "        \"Color to Move: {nxt-color} [/INST]\"\n",
    "    ),\n",
    "    \"completion\": \" <move>{move}</move> </s>\"\n",
    "}\n",
    "\n",
    "with open(\"data/template.json\", \"w\") as f:\n",
    "    json.dump(template, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3b7cd-8e7e-4a94-83c6-807b33451c74",
   "metadata": {},
   "source": [
    "In the final step of dataset preparation the training data, testing data, and prompt template will be uploaded to the S3 buckets previously initialized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76d50d9e-79a1-4a7d-aee8-d4dba26b645d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: s3://sagemaker-us-east-1-874604298668/datasets/3-1-8b\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "S3Uploader.upload(local_train_data_file, train_test_data_location)\n",
    "S3Uploader.upload(local_test_data_file, train_test_data_location)\n",
    "S3Uploader.upload(\"data/template.json\", train_test_data_location)\n",
    "\n",
    "print(f\"Training data: {train_test_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85e7322-fec1-4228-9874-10c83a00787b",
   "metadata": {},
   "source": [
    "### Submit the training job\n",
    "\n",
    "An estimator object is needed by Sagemaker to submit training jobs. In this estimator object there are a some items that should be taken note of:\n",
    "\n",
    "* The model id & version are being passed into the estimator.\n",
    "* The eula must be set to \"true\" - this is due to different LLM's on Sagemaker Jumpstart, being from different model providers. Each provider has its own eula.\n",
    "* instance type - this is the compute instance(s) being used to conduct the fine tuning job on.\n",
    "* The .fit method for the estimator is what actually submits the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "077696ae-c7aa-4aed-9998-94478f72614d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: meta-textgeneration-llama-3-1-8b-2024-11-25-22-41-52-754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-25 22:41:53 Starting - Starting the training job...\n",
      "2024-11-25 22:41:53 Pending - Training job waiting for capacity......\n",
      "2024-11-25 22:43:09 Pending - Preparing the instances for training......\n",
      "2024-11-25 22:44:22 Downloading - Downloading input data................................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-11-25 22:49:35,529 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-11-25 22:49:35,596 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-25 22:49:35,606 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-11-25 22:49:35,608 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\n",
      "2024-11-25 22:49:30 Training - Training image download completed. Training in progress.\u001b[34m2024-11-25 22:49:45,204 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.33.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/docstring-parser/docstring_parser-0.16-py3-none-any.whl (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/huggingface-hub/huggingface_hub-0.24.2-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cublas-cu12/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-cupti-cu12/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cudnn-cu12/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cufft-cu12/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-curand-cu12/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cusolver-cu12/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cusparse-cu12/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nccl-cu12/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nvjitlink-cu12/nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nvtx-cu12/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 29))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 30))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 31))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 32))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 33))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/shtab/shtab-1.7.1-py3-none-any.whl (from -r requirements.txt (line 34))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 35))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 36))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 37))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 38))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torch/torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (from -r requirements.txt (line 39))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.43.1-py3-none-any.whl (from -r requirements.txt (line 40))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/triton/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 41))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/trl/trl-0.8.1-py3-none-any.whl (from -r requirements.txt (line 42))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/typing-extensions/typing_extensions-4.8.0-py3-none-any.whl (from -r requirements.txt (line 43))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tyro/tyro-0.7.3-py3-none-any.whl (from -r requirements.txt (line 44))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 45))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.2.7-py2.py3-none-any.whl (from -r requirements.txt (line 46))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.14.1->-r requirements.txt (line 5)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 7)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.24.2->-r requirements.txt (line 8)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.43.1->-r requirements.txt (line 40)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro==0.7.3->-r requirements.txt (line 44)) (13.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (2.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0->-r requirements.txt (line 39)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0->-r requirements.txt (line 39)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (0.1.0)\u001b[0m\n",
      "\u001b[34mscipy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=d8ed1a1c22acc73d45d15b84ed2e7247464abd6e14f46bbe44b4bce783e8f046\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[34mSuccessfully built fire\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, Brotli, bitsandbytes, typing-extensions, triton, tokenize-rt, termcolor, shtab, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, safetensors, pyzstd, pyppmd, pycryptodomex, pybcj, pathspec, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, multivolumefile, loralib, inflate64, docstring-parser, py7zr, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, fire, black, tyro, tokenizers, nvidia-cusolver-cu12, transformers, torch, datasets, accelerate, trl, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing_extensions 4.7.1\u001b[0m\n",
      "\u001b[34mUninstalling typing_extensions-4.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing_extensions-4.7.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: triton\u001b[0m\n",
      "\u001b[34mFound existing installation: triton 2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mUninstalling triton-2.0.0.dev20221202:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled triton-2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.20.3\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.20.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.20.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.16.1\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.16.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.16.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 accelerate-0.33.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 docstring-parser-0.16 fire-0.5.0 huggingface-hub-0.24.2 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pyzstd-0.15.9 safetensors-0.4.2 sagemaker-jumpstart-huggingface-script-utilities-1.2.7 sagemaker-jumpstart-script-utilities-1.1.9 shtab-1.7.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 tokenizers-0.19.1 torch-2.2.0 transformers-4.43.1 triton-2.2.0 trl-0.8.1 typing-extensions-4.8.0 tyro-0.7.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-11-25 22:50:49,838 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-11-25 22:50:49,838 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-11-25 22:50:49,925 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-25 22:50:50,002 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-25 22:50:50,081 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-25 22:50:50,091 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"chat_template\": \"Llama3.1\",\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"1\",\n",
      "        \"instruction_tuned\": true,\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"target_modules\": \"q_proj,v_proj\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.48xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-3-1-8b-2024-11-25-22-41-52-754\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 192,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.48xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"chat_template\":\"Llama3.1\",\"enable_fsdp\":\"True\",\"epoch\":\"1\",\"instruction_tuned\":true,\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"target_modules\":\"q_proj,v_proj\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.48xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=192\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.48xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"chat_template\":\"Llama3.1\",\"enable_fsdp\":\"True\",\"epoch\":\"1\",\"instruction_tuned\":true,\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"target_modules\":\"q_proj,v_proj\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-3-1-8b-2024-11-25-22-41-52-754\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--chat_dataset\",\"False\",\"--chat_template\",\"Llama3.1\",\"--enable_fsdp\",\"True\",\"--epoch\",\"1\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--target_modules\",\"q_proj,v_proj\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_DATASET=False\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_TEMPLATE=Llama3.1\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=1\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=true\u001b[0m\n",
      "\u001b[34mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TARGET_MODULES=q_proj,v_proj\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --chat_dataset False --chat_template Llama3.1 --enable_fsdp True --epoch 1 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --target_modules q_proj,v_proj --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34m2024-11-25 22:50:50,123 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '8', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '8', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '1', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--target_modules', 'q_proj,v_proj', '--chat_template', 'Llama3.1', '--enable_fsdp', '--add_input_output_demarcation_key', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[34m[2024-11-25 22:50:55,260] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-11-25 22:50:55,260] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-11-25 22:50:55,260] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-11-25 22:50:55,260] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 6. Rank is 6\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 6\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 1. Rank is 1\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 1\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 5. Rank is 5\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 5\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 4. Rank is 4\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 4\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[34m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 3. Rank is 3\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 3\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 2. Rank is 2\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 2\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 7. Rank is 7\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 7\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 11397.57it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1090.00it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10014 examples [00:00, 660467.34 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 1820/10014 [00:00<00:00, 18089.31 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 3717/10014 [00:00<00:00, 18597.43 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]INFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▌    | 5630/10014 [00:00<00:00, 18835.27 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 1718/10014 [00:00<00:00, 17061.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▊        | 1869/10014 [00:00<00:00, 18577.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 1765/10014 [00:00<00:00, 17544.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 1850/10014 [00:00<00:00, 18386.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 1695/10014 [00:00<00:00, 16841.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 1849/10014 [00:00<00:00, 18377.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|███████▌  | 7529/10014 [00:00<00:00, 18892.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▍      | 3470/10014 [00:00<00:00, 17328.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 1786/10014 [00:00<00:00, 17755.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 3758/10014 [00:00<00:00, 18755.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  36%|███▌      | 3596/10014 [00:00<00:00, 17986.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 3728/10014 [00:00<00:00, 18613.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 3427/10014 [00:00<00:00, 17119.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 3756/10014 [00:00<00:00, 18776.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 5218/10014 [00:00<00:00, 17394.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▋    | 5654/10014 [00:00<00:00, 18844.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 5421/10014 [00:00<00:00, 18100.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▌    | 5611/10014 [00:00<00:00, 18707.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:00<00:00, 18824.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:00<00:00, 18731.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  51%|█████▏    | 5157/10014 [00:00<00:00, 17197.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|████▍     | 4457/10014 [00:00<00:00, 17779.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  57%|█████▋    | 5675/10014 [00:00<00:00, 18959.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 6990/10014 [00:00<00:00, 17518.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|███████▌  | 7549/10014 [00:00<00:00, 18884.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 7241/10014 [00:00<00:00, 18136.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|███████▍  | 7500/10014 [00:00<00:00, 18778.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  69%|██████▉   | 6901/10014 [00:00<00:00, 17287.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|██████▎   | 6277/10014 [00:00<00:00, 17944.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  76%|███████▌  | 7605/10014 [00:00<00:00, 19089.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▍| 9450/10014 [00:00<00:00, 18924.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  91%|█████████ | 9079/10014 [00:00<00:00, 18219.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:00<00:00, 18824.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:00<00:00, 18822.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:00<00:00, 18717.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▌| 9618/10014 [00:00<00:00, 17513.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 8094/10014 [00:00<00:00, 18021.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▌| 9533/10014 [00:00<00:00, 19152.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:00<00:00, 18103.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:00<00:00, 17421.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 1000/10014 [00:00<00:01, 8480.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:00<00:00, 18958.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▍| 9461/10014 [00:00<00:00, 17179.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:00<00:00, 17163.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  99%|█████████▉| 9931/10014 [00:00<00:00, 18114.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:00<00:00, 17945.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 3000/10014 [00:00<00:00, 11482.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 1000/10014 [00:00<00:01, 6668.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 1000/10014 [00:00<00:01, 6090.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 1000/10014 [00:00<00:01, 5818.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 1000/10014 [00:00<00:01, 5920.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 1000/10014 [00:00<00:01, 5835.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 5000/10014 [00:00<00:00, 10600.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 2000/10014 [00:00<00:01, 7453.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 2000/10014 [00:00<00:01, 7293.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 2000/10014 [00:00<00:01, 6968.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 2000/10014 [00:00<00:01, 6932.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 3000/10014 [00:00<00:00, 8609.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 3000/10014 [00:00<00:00, 8188.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 3000/10014 [00:00<00:00, 8249.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 1000/10014 [00:00<00:02, 3569.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 3000/10014 [00:00<00:00, 7866.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 7000/10014 [00:00<00:00, 10185.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 3000/10014 [00:00<00:00, 8015.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 1000/10014 [00:00<00:02, 3181.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 4000/10014 [00:00<00:00, 8602.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 4000/10014 [00:00<00:00, 8206.52 examples/s]#015Map:  40%|███▉      | 4000/10014 [00:00<00:00, 8388.19 examples/s]#015Map:  20%|█▉        | 2000/10014 [00:00<00:01, 5531.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 4000/10014 [00:00<00:00, 8242.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 4000/10014 [00:00<00:00, 8011.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 2000/10014 [00:00<00:01, 4537.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 5000/10014 [00:00<00:00, 8598.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 5000/10014 [00:00<00:00, 8686.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 5000/10014 [00:00<00:00, 8458.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 3000/10014 [00:00<00:01, 6733.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 5000/10014 [00:00<00:00, 8628.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 5000/10014 [00:00<00:00, 7956.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 6000/10014 [00:00<00:00, 8515.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 6000/10014 [00:00<00:00, 8427.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 4000/10014 [00:00<00:00, 6991.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 3000/10014 [00:00<00:01, 5367.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 6000/10014 [00:00<00:00, 7823.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 6000/10014 [00:00<00:00, 8453.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 6000/10014 [00:00<00:00, 8349.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 7000/10014 [00:00<00:00, 8465.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 5000/10014 [00:00<00:00, 7416.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 7000/10014 [00:00<00:00, 7733.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 4000/10014 [00:00<00:01, 5975.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 7000/10014 [00:00<00:00, 8554.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 7000/10014 [00:00<00:00, 7595.75 examples/s]#015Map:  70%|██████▉   | 7000/10014 [00:00<00:00, 8822.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 8000/10014 [00:00<00:00, 8088.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 6000/10014 [00:00<00:00, 7404.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 9000/10014 [00:01<00:00, 5934.20 examples/s] #015Map:  80%|███████▉  | 8000/10014 [00:01<00:00, 7953.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 5000/10014 [00:00<00:00, 6589.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 8000/10014 [00:00<00:00, 8538.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 8000/10014 [00:01<00:00, 7539.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 8000/10014 [00:00<00:00, 8299.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 9000/10014 [00:01<00:00, 7989.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 6000/10014 [00:01<00:00, 7044.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 10000/10014 [00:01<00:00, 6228.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 9000/10014 [00:01<00:00, 7783.61 examples/s]#015Map:  90%|████████▉ | 9000/10014 [00:01<00:00, 7761.71 examples/s]#015Map:  90%|████████▉ | 9000/10014 [00:01<00:00, 8508.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 9000/10014 [00:01<00:00, 8126.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:01<00:00, 7161.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 7000/10014 [00:01<00:00, 6619.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 10000/10014 [00:01<00:00, 8239.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:01<00:00, 8107.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 10000/10014 [00:01<00:00, 8274.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 10000/10014 [00:01<00:00, 8162.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:01<00:00, 8039.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:01<00:00, 7936.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 10000/10014 [00:01<00:00, 8274.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 7000/10014 [00:01<00:00, 7232.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:01<00:00, 8096.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 10000/10014 [00:01<00:00, 8093.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:01<00:00, 7997.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 1000/10014 [00:00<00:01, 5335.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 9000/10014 [00:01<00:00, 8322.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 1000/10014 [00:00<00:01, 5576.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 1000/10014 [00:00<00:01, 5690.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 9000/10014 [00:01<00:00, 8717.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 1000/10014 [00:00<00:01, 5312.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 1000/10014 [00:00<00:01, 5568.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:01<00:00, 8657.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:01<00:00, 7388.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 1000/10014 [00:00<00:01, 5574.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 2000/10014 [00:00<00:01, 5577.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:01<00:00, 7185.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10014 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 2000/10014 [00:00<00:01, 5890.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 2000/10014 [00:00<00:01, 5930.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 2000/10014 [00:00<00:01, 5577.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 2000/10014 [00:00<00:01, 5823.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 2000/10014 [00:00<00:01, 6045.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 1000/10014 [00:00<00:01, 5883.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 3000/10014 [00:00<00:01, 5817.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 1000/10014 [00:00<00:01, 5752.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 3000/10014 [00:00<00:01, 6062.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 3000/10014 [00:00<00:01, 6107.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 3000/10014 [00:00<00:01, 5958.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 3000/10014 [00:00<00:01, 6208.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 3000/10014 [00:00<00:01, 5723.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 2000/10014 [00:00<00:01, 5973.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 4000/10014 [00:00<00:01, 5966.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 2000/10014 [00:00<00:01, 5988.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 4000/10014 [00:00<00:00, 6169.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 4000/10014 [00:00<00:00, 6216.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 4000/10014 [00:00<00:00, 6321.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 4000/10014 [00:00<00:01, 6013.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 4000/10014 [00:00<00:01, 5830.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 3000/10014 [00:00<00:01, 6049.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 5000/10014 [00:00<00:00, 6058.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 3000/10014 [00:00<00:01, 6030.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 5000/10014 [00:00<00:00, 6217.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 5000/10014 [00:00<00:00, 6257.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 5000/10014 [00:00<00:00, 6378.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 5000/10014 [00:00<00:00, 6052.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 5000/10014 [00:00<00:00, 5892.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 4000/10014 [00:00<00:00, 6065.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 6000/10014 [00:01<00:00, 6110.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 4000/10014 [00:00<00:00, 6059.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 6000/10014 [00:00<00:00, 6244.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 6000/10014 [00:00<00:00, 6272.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 6000/10014 [00:00<00:00, 6408.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 6000/10014 [00:00<00:00, 6101.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 6000/10014 [00:01<00:00, 5906.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 5000/10014 [00:00<00:00, 6090.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 7000/10014 [00:01<00:00, 6135.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 5000/10014 [00:00<00:00, 6078.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 7000/10014 [00:01<00:00, 6252.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 7000/10014 [00:01<00:00, 6292.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 7000/10014 [00:01<00:00, 6427.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 7000/10014 [00:01<00:00, 6083.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 6000/10014 [00:00<00:00, 6101.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 7000/10014 [00:01<00:00, 5920.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 8000/10014 [00:01<00:00, 6174.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 6000/10014 [00:00<00:00, 6072.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 8000/10014 [00:01<00:00, 6267.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 8000/10014 [00:01<00:00, 6327.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 8000/10014 [00:01<00:00, 6465.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 8000/10014 [00:01<00:00, 6088.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 7000/10014 [00:01<00:00, 6106.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 8000/10014 [00:01<00:00, 5942.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 9000/10014 [00:01<00:00, 6170.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 7000/10014 [00:01<00:00, 6081.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 9000/10014 [00:01<00:00, 6278.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 9000/10014 [00:01<00:00, 6347.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 9000/10014 [00:01<00:00, 6476.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 9000/10014 [00:01<00:00, 6107.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 8000/10014 [00:01<00:00, 6129.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 9000/10014 [00:01<00:00, 5954.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 10000/10014 [00:01<00:00, 6199.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:01<00:00, 6046.64 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 10000/10014 [00:01<00:00, 6292.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 8000/10014 [00:01<00:00, 6123.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:01<00:00, 6187.47 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 10000/10014 [00:01<00:00, 6352.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 10000/10014 [00:01<00:00, 6493.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:01<00:00, 6243.05 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:01<00:00, 6362.47 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 10000/10014 [00:01<00:00, 6123.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:01<00:00, 6039.90 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 9000/10014 [00:01<00:00, 6108.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 10000/10014 [00:01<00:00, 5966.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:01<00:00, 5864.11 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 9000/10014 [00:01<00:00, 6094.30 examples/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 10000/10014 [00:01<00:00, 6118.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:01<00:00, 6076.26 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 10000/10014 [00:01<00:00, 6124.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10014/10014 [00:01<00:00, 6065.35 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:16,  5.64s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.32s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:19,  6.46s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:19,  6.58s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.70s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:19,  6.51s/it]#015Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.26s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:19,  6.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.04s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.72s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.67s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.70s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.64s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:12<00:13,  6.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.67s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.68s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.13s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.62s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.61s/it]#015Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.70s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.58s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.47s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.14s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.58s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.32s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.63s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.29s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.58s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.26s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.56s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.30s/it]\u001b[0m\n",
      "\u001b[34m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34m--> /opt/ml/additonals3data has 8030.261248 Million params\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.32s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.28s/it]\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/35 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/35 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mINFO:root:--> Training Set Length = 1116\u001b[0m\n",
      "\u001b[34mINFO:root:--> Validation Set Length = 280\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/35 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.3+cuda12.3\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/35 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/35 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/35 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/35 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/35 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 1/35 [00:29<16:42, 29.48s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 0.9303193092346191\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 1/35 [00:30<17:32, 30.94s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 1/35 [00:30<17:27, 30.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 1/35 [00:31<17:46, 31.38s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 1/35 [00:29<16:47, 29.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 1/35 [00:30<17:11, 30.34s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 1/35 [00:34<19:18, 34.06s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 1/35 [00:30<17:00, 30.00s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 2/35 [00:58<15:55, 28.94s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 2/35 [00:59<16:20, 29.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 2/35 [00:58<16:06, 29.30s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 2/35 [00:58<16:02, 29.16s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 0.8925336599349976\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 2/35 [00:59<16:15, 29.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 2/35 [00:58<15:57, 29.00s/it]#015Training Epoch0:   6%|#033[34m▌         #033[0m| 2/35 [01:02<16:57, 30.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 2/35 [00:59<16:13, 29.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 3/35 [01:26<15:19, 28.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 3/35 [01:28<15:32, 29.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 3/35 [01:27<15:25, 28.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 3/35 [01:27<15:22, 28.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 3/35 [01:27<15:28, 29.02s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 3/35 [01:26<15:20, 28.76s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 0.8748539090156555\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 3/35 [01:31<15:51, 29.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 3/35 [01:27<15:29, 29.05s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 4/35 [01:55<14:52, 28.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 4/35 [01:56<14:56, 28.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 4/35 [01:55<14:51, 28.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 4/35 [01:55<14:48, 28.67s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 0.8252168297767639\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 4/35 [01:55<14:49, 28.69s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 4/35 [01:56<14:55, 28.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 4/35 [01:59<15:08, 29.29s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 4/35 [01:56<14:54, 28.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 5/35 [02:23<14:19, 28.66s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 5/35 [02:23<14:19, 28.65s/it]#015Training Epoch0:  14%|#033[34m█▍        #033[0m| 5/35 [02:25<14:24, 28.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 5/35 [02:24<14:21, 28.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 5/35 [02:24<14:20, 28.70s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 0.8431819081306458\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 5/35 [02:25<14:23, 28.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 5/35 [02:25<14:23, 28.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 5/35 [02:28<14:31, 29.05s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 6/35 [02:52<13:49, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 6/35 [02:54<13:52, 28.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 6/35 [02:52<13:49, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 6/35 [02:53<13:50, 28.65s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 6/35 [02:52<13:50, 28.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 6/35 [02:53<13:51, 28.67s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 0.802839994430542\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 6/35 [02:53<13:51, 28.68s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 6/35 [02:56<13:56, 28.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 7/35 [03:20<13:20, 28.59s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 7/35 [03:20<13:20, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 7/35 [03:22<13:22, 28.67s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 7/35 [03:21<13:21, 28.63s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 0.8428230881690979\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 7/35 [03:22<13:22, 28.65s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 7/35 [03:21<13:21, 28.61s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 7/35 [03:22<13:22, 28.65s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 7/35 [03:25<13:25, 28.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 8/35 [03:49<12:51, 28.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 8/35 [03:51<12:52, 28.62s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 8/35 [03:50<12:51, 28.59s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 0.7658576965332031\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 8/35 [03:50<12:52, 28.61s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 8/35 [03:49<12:51, 28.58s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 8/35 [03:50<12:52, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 8/35 [03:49<12:51, 28.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 8/35 [03:53<12:54, 28.69s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 9/35 [04:17<12:22, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 9/35 [04:18<12:22, 28.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 9/35 [04:17<12:22, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 9/35 [04:19<12:23, 28.59s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 0.7545466423034668\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 9/35 [04:19<12:23, 28.58s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 9/35 [04:19<12:23, 28.58s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 9/35 [04:18<12:22, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 9/35 [04:22<12:24, 28.64s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 10/35 [04:46<11:53, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 10/35 [04:46<11:53, 28.54s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 0.8286449313163757\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 10/35 [04:48<11:54, 28.57s/it]#015Training Epoch0:  29%|#033[34m██▊       #033[0m| 10/35 [04:47<11:54, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 10/35 [04:47<11:54, 28.56s/it]#015Training Epoch0:  29%|#033[34m██▊       #033[0m| 10/35 [04:46<11:53, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 10/35 [04:50<11:55, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 10/35 [04:47<11:53, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 11/35 [05:15<11:27, 28.66s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 11/35 [05:15<11:27, 28.66s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 0.779940664768219\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 11/35 [05:16<11:28, 28.67s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 11/35 [05:16<11:28, 28.67s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 11/35 [05:17<11:28, 28.67s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 11/35 [05:19<11:28, 28.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 11/35 [05:16<11:27, 28.66s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 11/35 [05:15<11:27, 28.66s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 12/35 [05:43<10:58, 28.65s/it]#015Training Epoch0:  34%|#033[34m███▍      #033[0m| 12/35 [05:44<10:58, 28.65s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 0.7243335247039795\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 12/35 [05:45<10:59, 28.66s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 12/35 [05:45<10:59, 28.66s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 12/35 [05:45<10:59, 28.66s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 12/35 [05:44<10:59, 28.65s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 12/35 [05:44<10:59, 28.66s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 12/35 [05:48<10:59, 28.68s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 13/35 [06:12<10:29, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 13/35 [06:12<10:29, 28.60s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 0.7562045454978943\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 13/35 [06:13<10:29, 28.61s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 13/35 [06:13<10:29, 28.61s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 13/35 [06:14<10:29, 28.61s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 13/35 [06:12<10:29, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 13/35 [06:13<10:29, 28.61s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 13/35 [06:16<10:29, 28.62s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 14/35 [06:41<10:00, 28.59s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 14/35 [06:40<10:00, 28.59s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 14/35 [06:42<10:00, 28.60s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 0.7320687174797058\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 14/35 [06:42<10:00, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 14/35 [06:41<10:00, 28.59s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 14/35 [06:45<10:00, 28.61s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 14/35 [06:42<10:00, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 14/35 [06:41<10:00, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 15/35 [07:09<09:32, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 15/35 [07:11<09:32, 28.61s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 0.6965824365615845\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 15/35 [07:11<09:32, 28.61s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 15/35 [07:10<09:32, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 15/35 [07:09<09:32, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 15/35 [07:14<09:32, 28.61s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 15/35 [07:10<09:32, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 15/35 [07:10<09:32, 28.61s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 16/35 [07:38<09:03, 28.59s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 0.7188894748687744\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 16/35 [07:40<09:03, 28.59s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 16/35 [07:39<09:03, 28.59s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 16/35 [07:38<09:03, 28.59s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 16/35 [07:38<09:03, 28.59s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 16/35 [07:42<09:03, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 16/35 [07:39<09:03, 28.59s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 16/35 [07:39<09:03, 28.59s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▊     #033[0m| 17/35 [08:08<08:34, 28.58s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▊     #033[0m| 17/35 [08:07<08:34, 28.58s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▊     #033[0m| 17/35 [08:06<08:34, 28.58s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 0.7077888250350952\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▊     #033[0m| 17/35 [08:08<08:34, 28.58s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▊     #033[0m| 17/35 [08:06<08:34, 28.58s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▊     #033[0m| 17/35 [08:11<08:34, 28.59s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▊     #033[0m| 17/35 [08:08<08:34, 28.58s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▊     #033[0m| 17/35 [08:07<08:34, 28.58s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████▏    #033[0m| 18/35 [08:37<08:05, 28.58s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████▏    #033[0m| 18/35 [08:35<08:05, 28.58s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████▏    #033[0m| 18/35 [08:35<08:05, 28.58s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████▏    #033[0m| 18/35 [08:35<08:05, 28.58s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████▏    #033[0m| 18/35 [08:39<08:05, 28.58s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 0.6919945478439331\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████▏    #033[0m| 18/35 [08:36<08:05, 28.58s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████▏    #033[0m| 18/35 [08:36<08:05, 28.58s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████▏    #033[0m| 18/35 [08:36<08:05, 28.58s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 19/35 [09:03<07:37, 28.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 19/35 [09:04<07:37, 28.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 19/35 [09:05<07:37, 28.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 19/35 [09:08<07:37, 28.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 19/35 [09:03<07:37, 28.57s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 0.625791072845459\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 19/35 [09:05<07:37, 28.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 19/35 [09:05<07:37, 28.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 19/35 [09:04<07:37, 28.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 20/35 [09:34<07:08, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 20/35 [09:32<07:08, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 20/35 [09:32<07:08, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 20/35 [09:32<07:08, 28.56s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 0.6589822173118591\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 20/35 [09:33<07:08, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 20/35 [09:36<07:08, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 20/35 [09:33<07:08, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 20/35 [09:33<07:08, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 21/35 [10:00<06:39, 28.55s/it]#015Training Epoch0:  60%|#033[34m██████    #033[0m| 21/35 [10:02<06:39, 28.56s/it]#015Training Epoch0:  60%|#033[34m██████    #033[0m| 21/35 [10:01<06:39, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 21/35 [10:01<06:39, 28.55s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 0.6178299784660339\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 21/35 [10:02<06:39, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 21/35 [10:05<06:39, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 21/35 [10:02<06:39, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 21/35 [10:01<06:39, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 22/35 [10:30<06:12, 28.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 22/35 [10:31<06:12, 28.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 22/35 [10:29<06:12, 28.63s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 0.6172731518745422\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 22/35 [10:31<06:12, 28.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 22/35 [10:29<06:12, 28.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 22/35 [10:31<06:12, 28.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 22/35 [10:34<06:12, 28.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 22/35 [10:30<06:12, 28.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 23/35 [11:00<05:43, 28.60s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 0.6043761968612671\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 23/35 [10:59<05:43, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 23/35 [10:58<05:43, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 23/35 [10:58<05:43, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 23/35 [10:58<05:43, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 23/35 [11:02<05:43, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 23/35 [10:59<05:43, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 23/35 [10:59<05:43, 28.60s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 0.5935969948768616\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▊   #033[0m| 24/35 [11:28<05:14, 28.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▊   #033[0m| 24/35 [11:28<05:14, 28.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▊   #033[0m| 24/35 [11:27<05:14, 28.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▊   #033[0m| 24/35 [11:26<05:14, 28.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▊   #033[0m| 24/35 [11:26<05:14, 28.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▊   #033[0m| 24/35 [11:31<05:14, 28.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▊   #033[0m| 24/35 [11:27<05:14, 28.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▊   #033[0m| 24/35 [11:28<05:14, 28.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 25/35 [11:55<04:45, 28.56s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 0.5935535430908203\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 25/35 [11:55<04:45, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 25/35 [11:56<04:45, 28.56s/it]#015Training Epoch0:  71%|#033[34m███████▏  #033[0m| 25/35 [11:55<04:45, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 25/35 [11:59<04:45, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 25/35 [11:57<04:45, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 25/35 [11:56<04:45, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 25/35 [11:56<04:45, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 26/35 [12:23<04:16, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 26/35 [12:25<04:16, 28.55s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 0.5675958395004272\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 26/35 [12:23<04:16, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 26/35 [12:28<04:16, 28.55s/it]#015Training Epoch0:  74%|#033[34m███████▍  #033[0m| 26/35 [12:25<04:16, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 26/35 [12:24<04:16, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 26/35 [12:25<04:16, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 26/35 [12:24<04:16, 28.55s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 0.5771690011024475\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 27/35 [12:52<03:48, 28.56s/it]#015Training Epoch0:  77%|#033[34m███████▋  #033[0m| 27/35 [12:53<03:48, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 27/35 [12:54<03:48, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 27/35 [12:52<03:48, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 27/35 [12:56<03:48, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 27/35 [12:53<03:48, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 27/35 [12:53<03:48, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 27/35 [12:52<03:48, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 28/35 [13:21<03:19, 28.55s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 0.5494174957275391\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 28/35 [13:22<03:19, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 28/35 [13:22<03:19, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 28/35 [13:25<03:19, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 28/35 [13:20<03:19, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 28/35 [13:21<03:19, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 28/35 [13:22<03:19, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 28/35 [13:21<03:19, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 29/35 [13:49<02:51, 28.54s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 0.574393093585968\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 29/35 [13:49<02:51, 28.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 29/35 [13:50<02:51, 28.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 29/35 [13:51<02:51, 28.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 29/35 [13:54<02:51, 28.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 29/35 [13:50<02:51, 28.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 29/35 [13:49<02:51, 28.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 29/35 [13:50<02:51, 28.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 30/35 [14:18<02:22, 28.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 30/35 [14:19<02:22, 28.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 30/35 [14:17<02:22, 28.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 30/35 [14:18<02:22, 28.54s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 0.5164603590965271\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 30/35 [14:19<02:22, 28.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 30/35 [14:19<02:22, 28.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 30/35 [14:22<02:22, 28.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 30/35 [14:18<02:22, 28.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▊ #033[0m| 31/35 [14:46<01:54, 28.56s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 0.5085524916648865\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▊ #033[0m| 31/35 [14:48<01:54, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▊ #033[0m| 31/35 [14:46<01:54, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▊ #033[0m| 31/35 [14:47<01:54, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▊ #033[0m| 31/35 [14:48<01:54, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▊ #033[0m| 31/35 [14:51<01:54, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▊ #033[0m| 31/35 [14:47<01:54, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▊ #033[0m| 31/35 [14:47<01:54, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████▏#033[0m| 32/35 [15:15<01:25, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████▏#033[0m| 32/35 [15:15<01:25, 28.55s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 0.5202111005783081\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████▏#033[0m| 32/35 [15:17<01:25, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████▏#033[0m| 32/35 [15:16<01:25, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████▏#033[0m| 32/35 [15:15<01:25, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████▏#033[0m| 32/35 [15:19<01:25, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████▏#033[0m| 32/35 [15:16<01:25, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████▏#033[0m| 32/35 [15:15<01:25, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 33/35 [15:43<00:57, 28.55s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 0.5305137038230896\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 33/35 [15:45<00:57, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 33/35 [15:45<00:57, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 33/35 [15:44<00:57, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 33/35 [15:48<00:57, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 33/35 [15:44<00:57, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 33/35 [15:43<00:57, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 33/35 [15:44<00:57, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 34/35 [16:12<00:28, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 34/35 [16:16<00:28, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 34/35 [16:12<00:28, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 34/35 [16:14<00:28, 28.55s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 0.5365814566612244\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 34/35 [16:13<00:28, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 34/35 [16:13<00:28, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 34/35 [16:12<00:28, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 34/35 [16:13<00:28, 28.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 35/35 [16:40<00:00, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 35/35 [16:40<00:00, 28.60s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 35/35 [16:45<00:00, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 35/35 [16:45<00:00, 28.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 35/35 [16:42<00:00, 28.56s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 0.5348081588745117\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 35/35 [16:42<00:00, 28.65s/it]\u001b[0m\n",
      "\u001b[34m#015Training Epoch0: 100%|#033[34m██████████#033[0m| 35/35 [16:40<00:00, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 35/35 [16:42<00:00, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 35/35 [16:40<00:00, 28.59s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 35/35 [16:41<00:00, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 35/35 [16:42<00:00, 28.64s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 35/35 [16:41<00:00, 28.61s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 35/35 [16:42<00:00, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 35/35 [16:42<00:00, 28.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 35/35 [16:41<00:00, 28.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 35/35 [16:41<00:00, 28.62s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/35 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 11 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 13 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 11 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 4 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/35 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/35 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/35 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/35 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/35 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\u001b[0m\n",
      "\u001b[34mWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\u001b[0m\n",
      "\u001b[34mWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\u001b[0m\n",
      "\u001b[34mWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\u001b[0m\n",
      "\u001b[34mWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\u001b[0m\n",
      "\u001b[34mWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/35 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/35 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/35 [00:14<08:11, 14.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/35 [00:14<08:11, 14.44s/it]#015evaluating Epoch:   3%|#033[32m▎         #033[0m| 1/35 [00:14<08:11, 14.45s/it]#015evaluating Epoch:   3%|#033[32m▎         #033[0m| 1/35 [00:14<08:11, 14.44s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/35 [00:14<08:01, 14.17s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/35 [00:14<08:02, 14.18s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/35 [00:14<08:11, 14.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/35 [00:14<08:10, 14.43s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▌         #033[0m| 2/35 [00:26<07:15, 13.21s/it]#015evaluating Epoch:   6%|#033[32m▌         #033[0m| 2/35 [00:26<07:15, 13.20s/it]#015evaluating Epoch:   6%|#033[32m▌         #033[0m| 2/35 [00:26<07:15, 13.20s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▌         #033[0m| 2/35 [00:26<07:15, 13.20s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▌         #033[0m| 2/35 [00:26<07:11, 13.08s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▌         #033[0m| 2/35 [00:26<07:11, 13.09s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▌         #033[0m| 2/35 [00:26<07:15, 13.20s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▌         #033[0m| 2/35 [00:26<07:15, 13.19s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 3/35 [00:39<06:49, 12.80s/it]#015evaluating Epoch:   9%|#033[32m▊         #033[0m| 3/35 [00:39<06:49, 12.80s/it]#015evaluating Epoch:   9%|#033[32m▊         #033[0m| 3/35 [00:39<06:49, 12.80s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 3/35 [00:38<06:47, 12.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 3/35 [00:38<06:47, 12.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 3/35 [00:39<06:49, 12.80s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 3/35 [00:39<06:49, 12.80s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 3/35 [00:39<06:49, 12.80s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█▏        #033[0m| 4/35 [00:51<06:30, 12.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█▏        #033[0m| 4/35 [00:51<06:30, 12.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█▏        #033[0m| 4/35 [00:51<06:30, 12.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█▏        #033[0m| 4/35 [00:51<06:29, 12.56s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█▏        #033[0m| 4/35 [00:51<06:29, 12.56s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█▏        #033[0m| 4/35 [00:51<06:30, 12.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█▏        #033[0m| 4/35 [00:51<06:30, 12.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█▏        #033[0m| 4/35 [00:51<06:30, 12.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 5/35 [01:03<06:14, 12.49s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 5/35 [01:03<06:14, 12.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 5/35 [01:03<06:13, 12.47s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 5/35 [01:03<06:14, 12.49s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 5/35 [01:03<06:14, 12.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 5/35 [01:03<06:13, 12.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 5/35 [01:03<06:14, 12.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 5/35 [01:03<06:14, 12.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 6/35 [01:15<06:00, 12.43s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 6/35 [01:15<06:00, 12.42s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 6/35 [01:15<06:00, 12.42s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 6/35 [01:15<05:59, 12.41s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 6/35 [01:15<05:59, 12.41s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 6/35 [01:15<06:00, 12.42s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 6/35 [01:15<06:00, 12.42s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 6/35 [01:15<06:00, 12.42s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m██        #033[0m| 7/35 [01:28<05:46, 12.39s/it]#015evaluating Epoch:  20%|#033[32m██        #033[0m| 7/35 [01:28<05:46, 12.39s/it]#015evaluating Epoch:  20%|#033[32m██        #033[0m| 7/35 [01:28<05:46, 12.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m██        #033[0m| 7/35 [01:28<05:46, 12.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m██        #033[0m| 7/35 [01:28<05:46, 12.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m██        #033[0m| 7/35 [01:28<05:46, 12.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m██        #033[0m| 7/35 [01:28<05:46, 12.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m██        #033[0m| 7/35 [01:28<05:46, 12.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 8/35 [01:40<05:33, 12.35s/it]#015evaluating Epoch:  23%|#033[32m██▎       #033[0m| 8/35 [01:40<05:33, 12.35s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 8/35 [01:40<05:33, 12.35s/it]#015evaluating Epoch:  23%|#033[32m██▎       #033[0m| 8/35 [01:40<05:33, 12.35s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 8/35 [01:40<05:33, 12.35s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 8/35 [01:40<05:33, 12.35s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 8/35 [01:40<05:33, 12.35s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 8/35 [01:40<05:33, 12.35s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 9/35 [01:52<05:20, 12.34s/it]#015evaluating Epoch:  26%|#033[32m██▌       #033[0m| 9/35 [01:52<05:20, 12.34s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 9/35 [01:52<05:20, 12.34s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 9/35 [01:52<05:20, 12.34s/it]#015evaluating Epoch:  26%|#033[32m██▌       #033[0m| 9/35 [01:52<05:20, 12.33s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 9/35 [01:52<05:20, 12.33s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 9/35 [01:52<05:20, 12.34s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 9/35 [01:52<05:20, 12.34s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 10/35 [02:05<05:08, 12.32s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 10/35 [02:05<05:08, 12.32s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 10/35 [02:04<05:08, 12.32s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 10/35 [02:04<05:08, 12.32s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 10/35 [02:05<05:08, 12.32s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 10/35 [02:05<05:08, 12.32s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 10/35 [02:05<05:08, 12.32s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 10/35 [02:05<05:08, 12.32s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███▏      #033[0m| 11/35 [02:17<04:55, 12.30s/it]#015evaluating Epoch:  31%|#033[32m███▏      #033[0m| 11/35 [02:17<04:55, 12.30s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███▏      #033[0m| 11/35 [02:17<04:55, 12.30s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███▏      #033[0m| 11/35 [02:17<04:55, 12.30s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███▏      #033[0m| 11/35 [02:17<04:55, 12.30s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███▏      #033[0m| 11/35 [02:17<04:55, 12.30s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███▏      #033[0m| 11/35 [02:17<04:55, 12.30s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███▏      #033[0m| 11/35 [02:17<04:55, 12.30s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 12/35 [02:29<04:42, 12.30s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 12/35 [02:29<04:42, 12.30s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 12/35 [02:29<04:42, 12.30s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 12/35 [02:29<04:42, 12.30s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 12/35 [02:29<04:42, 12.30s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 12/35 [02:29<04:42, 12.30s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 12/35 [02:29<04:42, 12.30s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 12/35 [02:29<04:42, 12.30s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 13/35 [02:41<04:30, 12.29s/it]#015evaluating Epoch:  37%|#033[32m███▋      #033[0m| 13/35 [02:41<04:30, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 13/35 [02:41<04:30, 12.29s/it]#015evaluating Epoch:  37%|#033[32m███▋      #033[0m| 13/35 [02:41<04:30, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 13/35 [02:41<04:30, 12.29s/it]#015evaluating Epoch:  37%|#033[32m███▋      #033[0m| 13/35 [02:41<04:30, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 13/35 [02:41<04:30, 12.29s/it]#015evaluating Epoch:  37%|#033[32m███▋      #033[0m| 13/35 [02:41<04:30, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 14/35 [02:54<04:17, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 14/35 [02:54<04:17, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 14/35 [02:54<04:17, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 14/35 [02:54<04:17, 12.27s/it]#015evaluating Epoch:  40%|#033[32m████      #033[0m| 14/35 [02:53<04:17, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 14/35 [02:53<04:17, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 14/35 [02:54<04:17, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 14/35 [02:54<04:17, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 15/35 [03:06<04:05, 12.28s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 15/35 [03:06<04:05, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 15/35 [03:06<04:05, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 15/35 [03:06<04:05, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 15/35 [03:06<04:05, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 15/35 [03:06<04:05, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 15/35 [03:06<04:05, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 15/35 [03:06<04:05, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▌     #033[0m| 16/35 [03:18<03:53, 12.28s/it]#015evaluating Epoch:  46%|#033[32m████▌     #033[0m| 16/35 [03:18<03:53, 12.28s/it]#015evaluating Epoch:  46%|#033[32m████▌     #033[0m| 16/35 [03:18<03:53, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▌     #033[0m| 16/35 [03:18<03:53, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▌     #033[0m| 16/35 [03:18<03:53, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▌     #033[0m| 16/35 [03:18<03:53, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▌     #033[0m| 16/35 [03:18<03:53, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▌     #033[0m| 16/35 [03:18<03:53, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▊     #033[0m| 17/35 [03:31<03:41, 12.28s/it]#015evaluating Epoch:  49%|#033[32m████▊     #033[0m| 17/35 [03:31<03:41, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▊     #033[0m| 17/35 [03:31<03:41, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▊     #033[0m| 17/35 [03:30<03:41, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▊     #033[0m| 17/35 [03:31<03:41, 12.28s/it]#015evaluating Epoch:  49%|#033[32m████▊     #033[0m| 17/35 [03:30<03:41, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▊     #033[0m| 17/35 [03:31<03:41, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▊     #033[0m| 17/35 [03:31<03:41, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████▏    #033[0m| 18/35 [03:43<03:28, 12.27s/it]#015evaluating Epoch:  51%|#033[32m█████▏    #033[0m| 18/35 [03:43<03:28, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████▏    #033[0m| 18/35 [03:43<03:28, 12.27s/it]#015evaluating Epoch:  51%|#033[32m█████▏    #033[0m| 18/35 [03:43<03:28, 12.27s/it]#015evaluating Epoch:  51%|#033[32m█████▏    #033[0m| 18/35 [03:43<03:28, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████▏    #033[0m| 18/35 [03:43<03:28, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████▏    #033[0m| 18/35 [03:43<03:28, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████▏    #033[0m| 18/35 [03:43<03:28, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▍    #033[0m| 19/35 [03:55<03:16, 12.27s/it]#015evaluating Epoch:  54%|#033[32m█████▍    #033[0m| 19/35 [03:55<03:16, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▍    #033[0m| 19/35 [03:55<03:16, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▍    #033[0m| 19/35 [03:55<03:16, 12.27s/it]#015evaluating Epoch:  54%|#033[32m█████▍    #033[0m| 19/35 [03:55<03:16, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▍    #033[0m| 19/35 [03:55<03:16, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▍    #033[0m| 19/35 [03:55<03:16, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▍    #033[0m| 19/35 [03:55<03:16, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 20/35 [04:07<03:04, 12.27s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 20/35 [04:07<03:04, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 20/35 [04:07<03:04, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 20/35 [04:07<03:04, 12.27s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 20/35 [04:07<03:04, 12.27s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 20/35 [04:07<03:04, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 20/35 [04:07<03:04, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 20/35 [04:07<03:04, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m██████    #033[0m| 21/35 [04:20<02:51, 12.26s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m██████    #033[0m| 21/35 [04:20<02:51, 12.26s/it]#015evaluating Epoch:  60%|#033[32m██████    #033[0m| 21/35 [04:19<02:51, 12.26s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m██████    #033[0m| 21/35 [04:19<02:51, 12.26s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m██████    #033[0m| 21/35 [04:20<02:51, 12.26s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m██████    #033[0m| 21/35 [04:20<02:51, 12.26s/it]#015evaluating Epoch:  60%|#033[32m██████    #033[0m| 21/35 [04:20<02:51, 12.26s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m██████    #033[0m| 21/35 [04:20<02:51, 12.26s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 22/35 [04:32<02:39, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 22/35 [04:32<02:39, 12.27s/it]#015evaluating Epoch:  63%|#033[32m██████▎   #033[0m| 22/35 [04:32<02:39, 12.27s/it]#015evaluating Epoch:  63%|#033[32m██████▎   #033[0m| 22/35 [04:32<02:39, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 22/35 [04:32<02:39, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 22/35 [04:32<02:39, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 22/35 [04:32<02:39, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 22/35 [04:32<02:39, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 23/35 [04:44<02:27, 12.27s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 23/35 [04:44<02:27, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 23/35 [04:44<02:27, 12.27s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 23/35 [04:44<02:27, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 23/35 [04:44<02:27, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 23/35 [04:44<02:27, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 23/35 [04:44<02:27, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 23/35 [04:44<02:27, 12.27s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▊   #033[0m| 24/35 [04:56<02:15, 12.28s/it]#015evaluating Epoch:  69%|#033[32m██████▊   #033[0m| 24/35 [04:56<02:15, 12.28s/it]#015evaluating Epoch:  69%|#033[32m██████▊   #033[0m| 24/35 [04:56<02:15, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▊   #033[0m| 24/35 [04:56<02:15, 12.28s/it]#015evaluating Epoch:  69%|#033[32m██████▊   #033[0m| 24/35 [04:56<02:15, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▊   #033[0m| 24/35 [04:56<02:15, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▊   #033[0m| 24/35 [04:56<02:15, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▊   #033[0m| 24/35 [04:56<02:15, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 25/35 [05:09<02:02, 12.28s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 25/35 [05:09<02:02, 12.28s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 25/35 [05:08<02:02, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 25/35 [05:09<02:02, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 25/35 [05:08<02:02, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 25/35 [05:09<02:02, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 25/35 [05:09<02:02, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 25/35 [05:09<02:02, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 26/35 [05:21<01:50, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 26/35 [05:21<01:50, 12.29s/it]#015evaluating Epoch:  74%|#033[32m███████▍  #033[0m| 26/35 [05:21<01:50, 12.29s/it]#015evaluating Epoch:  74%|#033[32m███████▍  #033[0m| 26/35 [05:21<01:50, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 26/35 [05:21<01:50, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 26/35 [05:21<01:50, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 26/35 [05:21<01:50, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 26/35 [05:21<01:50, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 27/35 [05:33<01:38, 12.29s/it]#015evaluating Epoch:  77%|#033[32m███████▋  #033[0m| 27/35 [05:33<01:38, 12.29s/it]#015evaluating Epoch:  77%|#033[32m███████▋  #033[0m| 27/35 [05:33<01:38, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 27/35 [05:33<01:38, 12.29s/it]#015evaluating Epoch:  77%|#033[32m███████▋  #033[0m| 27/35 [05:33<01:38, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 27/35 [05:33<01:38, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 27/35 [05:33<01:38, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 27/35 [05:33<01:38, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 28/35 [05:46<01:25, 12.28s/it]#015evaluating Epoch:  80%|#033[32m████████  #033[0m| 28/35 [05:46<01:25, 12.28s/it]#015evaluating Epoch:  80%|#033[32m████████  #033[0m| 28/35 [05:46<01:25, 12.28s/it]#015evaluating Epoch:  80%|#033[32m████████  #033[0m| 28/35 [05:45<01:25, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 28/35 [05:45<01:25, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 28/35 [05:46<01:25, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 28/35 [05:46<01:25, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 28/35 [05:46<01:25, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 29/35 [05:58<01:13, 12.29s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 29/35 [05:58<01:13, 12.29s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 29/35 [05:58<01:13, 12.29s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 29/35 [05:58<01:13, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 29/35 [05:58<01:13, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 29/35 [05:58<01:13, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 29/35 [05:58<01:13, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 29/35 [05:58<01:13, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 30/35 [06:10<01:01, 12.28s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 30/35 [06:10<01:01, 12.28s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 30/35 [06:10<01:01, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 30/35 [06:10<01:01, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 30/35 [06:10<01:01, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 30/35 [06:10<01:01, 12.28s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 30/35 [06:10<01:01, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 30/35 [06:10<01:01, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▊ #033[0m| 31/35 [06:22<00:49, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▊ #033[0m| 31/35 [06:22<00:49, 12.28s/it]#015evaluating Epoch:  89%|#033[32m████████▊ #033[0m| 31/35 [06:22<00:49, 12.28s/it]#015evaluating Epoch:  89%|#033[32m████████▊ #033[0m| 31/35 [06:22<00:49, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▊ #033[0m| 31/35 [06:22<00:49, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▊ #033[0m| 31/35 [06:22<00:49, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▊ #033[0m| 31/35 [06:22<00:49, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▊ #033[0m| 31/35 [06:22<00:49, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 32/35 [06:35<00:36, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 32/35 [06:35<00:36, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 32/35 [06:34<00:36, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 32/35 [06:35<00:36, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 32/35 [06:35<00:36, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 32/35 [06:35<00:36, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 32/35 [06:35<00:36, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 32/35 [06:34<00:36, 12.28s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▍#033[0m| 33/35 [06:47<00:24, 12.29s/it]#015evaluating Epoch:  94%|#033[32m█████████▍#033[0m| 33/35 [06:47<00:24, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▍#033[0m| 33/35 [06:47<00:24, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▍#033[0m| 33/35 [06:47<00:24, 12.29s/it]#015evaluating Epoch:  94%|#033[32m█████████▍#033[0m| 33/35 [06:47<00:24, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▍#033[0m| 33/35 [06:47<00:24, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▍#033[0m| 33/35 [06:47<00:24, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▍#033[0m| 33/35 [06:47<00:24, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 34/35 [06:59<00:12, 12.29s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 34/35 [06:59<00:12, 12.29s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 34/35 [06:59<00:12, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 34/35 [06:59<00:12, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 34/35 [06:59<00:12, 12.29s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 34/35 [06:59<00:12, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 34/35 [06:59<00:12, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 34/35 [06:59<00:12, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 35/35 [07:12<00:00, 12.30s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 35/35 [07:12<00:00, 12.30s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 35/35 [07:11<00:00, 12.30s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 35/35 [07:11<00:00, 12.29s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 35/35 [07:12<00:00, 12.35s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 35/35 [07:12<00:00, 12.35s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 35/35 [07:12<00:00, 12.30s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 35/35 [07:11<00:00, 12.34s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 35/35 [07:12<00:00, 12.30s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 35/35 [07:11<00:00, 12.34s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 35/35 [07:12<00:00, 12.35s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 35/35 [07:12<00:00, 12.35s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 35/35 [07:12<00:00, 12.30s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 35/35 [07:12<00:00, 12.35s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 35/35 [07:12<00:00, 12.30s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 35/35 [07:12<00:00, 12.35s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(1.6640, device='cuda:0') eval_epoch_loss=tensor(0.5092, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 0 is 0.5092483162879944\u001b[0m\n",
      "\u001b[34mEpoch 1: train_perplexity=1.9828, train_epoch_loss=0.6845, epcoh time 1002.690608858s\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_prep, Value: 1.9827733039855957\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_loss, Value: 0.6844965219497681\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_prep, Value: 1.6640398502349854\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_loss, Value: 0.5092483162879944\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_epoch_time, Value: 1002.690608858\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_checkpoint_time, Value: 12.42406799000014\u001b[0m\n",
      "\u001b[34mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  9.44it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  9.43it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  9.45it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.73it/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Saving the combined model in safetensors format.\u001b[0m\n",
      "\u001b[34mINFO:root:Saving complete.\u001b[0m\n",
      "\u001b[34mINFO:root:Copying tokenizer to the output directory.\u001b[0m\n",
      "\u001b[34mINFO:root:Putting inference code with the fine-tuned model directory.\u001b[0m\n",
      "\u001b[34m2024-11-25 23:17:20,279 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-11-25 23:17:20,279 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-11-25 23:17:20,280 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-11-25 23:17:28 Uploading - Uploading generated training model\n",
      "2024-11-25 23:18:06 Completed - Training job completed\n",
      "Training seconds: 2024\n",
      "Billable seconds: 2024\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    environment={\"accept_eula\": \"true\"},  # Please change this to {\"accept_eula\": \"true\"}\n",
    "    disable_output_compression=True,\n",
    "    instance_type=\"ml.g5.48xlarge\" #please change to \"ml.g5.48xlarge\" if max records are being used. For the default of 100 the default instance type set here is fine.\n",
    ")\n",
    "# By default, instruction tuning is set to false. Thus, to use instruction tuning dataset you use\n",
    "estimator.set_hyperparameters(instruction_tuned=True, epoch=\"1\", max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": train_test_data_location})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce09e6a2-6353-4f12-8a07-ba9d49b2e2ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S3DataSource': {'S3Uri': 's3://sagemaker-us-east-1-874604298668/meta-textgeneration-llama-3-1-8b-2024-11-25-22-41-52-754/output/model/',\n",
       "  'S3DataType': 'S3Prefix',\n",
       "  'CompressionType': 'None'}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get Model data \n",
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e709d760-2285-4991-8b6b-5f8264d53912",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'rivchess_model_src' (dict)\n"
     ]
    }
   ],
   "source": [
    "#Get the S3 path of the model. This S3 path will be used to \n",
    "rivchess_model_src = {\"s3DataSource\": {\"s3Uri\": f'{ estimator.model_data[\"S3DataSource\"][\"S3Uri\"] }'}}\n",
    "%store rivchess_model_src\n",
    "rivchess_model_src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee68a9e7-9d0b-4cd3-a5d4-6c8774b1f186",
   "metadata": {},
   "source": [
    "### Import the fine tuned model into Bedrock using Bedrock Custom Model Import (CMI)\n",
    "\n",
    "The fine tuned model will now be imported into Amazon Bedrock. This model will be able to be used via the invoke model & invoke model with response stream apis. This model will run in a serverless capacity. Take note that the first inference conducted after a 5 minute window of no inferences, will face a cold startup time. Subsequent inference requests will be conducted as normal, as long as there is less than a 5 minute window between them. To import the model an Import Job must be commenced, which is outlined below.\n",
    "\n",
    "If you would like to import via the console instead of programmatically, you may follow the instructions [here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model-job.html) using the S3 bucket location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09fd2cc9-3e6d-4e2f-a30a-480e648be36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "742d221f-9999-47f3-8c1f-0d76bdac0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "br_client = boto3.client('bedrock', region_name='us-east-1')\n",
    "br_run_client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "model_split = model_id.split(\"/\")\n",
    "if len(model_split) > 1:\n",
    "    rivchess_model_nm = f\"RIVCHESS-{model_split[1]}\"\n",
    "else:\n",
    "    rivchess_model_nm = f\"RIVCHESS-{model_split[0]}\"\n",
    "\n",
    "rivchess_imp_jb_nm = f\"{rivchess_model_nm}-job-{datetime.datetime.now().strftime('%Y%m%d%M%H%S')}\"\n",
    "role_arn = role\n",
    "\n",
    "create_model_import_job_resp = br_client.create_model_import_job(jobName=rivchess_imp_jb_nm,\n",
    "                                  importedModelName=rivchess_model_nm,\n",
    "                                  roleArn=role_arn,\n",
    "                                  modelDataSource=rivchess_model_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a85e1248-9ffb-4a2c-9ceb-b2208928e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dot timer\n",
    "import time\n",
    "def interactive_sleep(seconds: int):\n",
    "    dots = ''\n",
    "    for i in range(seconds):\n",
    "        dots += '.'\n",
    "        print(dots, end='\\r')\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4544ef6-da52-4f89-8e3e-99bb884d8827",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_model_import_jobs_response = br_client.list_model_import_jobs(\n",
    "    nameContains=rivchess_imp_jb_nm)\n",
    "\n",
    "print(f\"BR CMI Import Job - {create_model_import_job_resp['jobArn']} is - {list_model_import_jobs_response['modelImportJobSummaries'][0]['status']}\")\n",
    "while list_model_import_jobs_response['modelImportJobSummaries'][0]['status'] != 'Completed':\n",
    "    interactive_sleep(30)\n",
    "    list_model_import_jobs_response = br_client.list_model_import_jobs(nameContains=rivchess_imp_jb_nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20476bbc-9b2b-4df4-8016-d0d2527287c0",
   "metadata": {},
   "source": [
    "### Invoke the imported model using Bedrock API's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5778e6-d1e0-4d2d-9684-06bc410bfb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    get_imported_model_response = br_client.get_imported_model(\n",
    "        modelIdentifier=rivchess_model_nm\n",
    "    )\n",
    "\n",
    "    br_model_id = get_imported_model_response['modelArn']\n",
    "    br_model_id\n",
    "except br_client.exceptions.ResourceNotFoundException:\n",
    "    print(\"Model not yet imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f1812b-7ccd-405b-8e00-676ae4b80db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_invoke_model_and_print(native_request):\n",
    "    request = json.dumps(native_request)\n",
    "\n",
    "    try:\n",
    "        # Invoke the model with the request.\n",
    "        response = br_run_client.invoke_model(modelId=br_model_id, body=request)\n",
    "        model_response = json.loads(response[\"body\"].read())\n",
    "        # print(f\"model_response: {model_response}\")\n",
    "        response_text = model_response['generation'].replace(\"\\n\", \"\").replace(\"### Response:\", \"\")\n",
    "        return response_text\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{br_model_id}'. Reason: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9790b5-1d6a-49d8-ae59-03fd9d90382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a103cd-4a69-4307-9fb4-46e1529ebeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_color = \"BLACK\"\n",
    "board_fen = \"6k1/p1q1prbp/b3n1p1/2pPPp2/5P1Q/4BN2/Pr2N1PP/R1R4K b - - 0 21\"\n",
    "\n",
    "# Format the prompt using the template\n",
    "formatted_prompt = template[\"prompt\"].format(\n",
    "    fen=board_fen,\n",
    "    nxt_color=move_color\n",
    ")\n",
    "\n",
    "native_request = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"max_tokens\": 50,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.1\n",
    "}\n",
    "\n",
    "call_invoke_model_and_print(native_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24cc1fd-4022-4621-a192-785aa2167613",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_color = \"WHITE\"\n",
    "board_fen = \"1r1qk2r/p2bppbp/n2P1np1/1pp5/3P4/2P1BB2/PP2QPPP/RN2K1NR w KQk - 3 11\"\n",
    "\n",
    "# Format the prompt using the template\n",
    "formatted_prompt = template[\"prompt\"].format(\n",
    "    fen=board_fen,\n",
    "    nxt_color=move_color\n",
    ")\n",
    "\n",
    "native_request = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"max_tokens\": 100,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 50,\n",
    "}\n",
    "\n",
    "call_invoke_model_and_print(native_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb9e06-7481-4054-9a3f-c2b2442e6d4c",
   "metadata": {},
   "source": [
    "### Clean Up\n",
    "Once the model will no longer be used, it may be deleted with the cell below. If additional testing for the model needs to be done, the \"test_chess_model.ipynb\" notebook can be opened to test the model against a Chess engine called Stockfish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb94591-000e-4b58-a47b-a5d75d3856e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_imported_model_response = br_client.delete_imported_model(\n",
    "    modelIdentifier=br_model_id\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
